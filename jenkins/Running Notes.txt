Jenkins Notes:



1. ssh root@134.122.28.225  //command to connect to digital ocean server
2. systemctl status jenkins //command to check whether jenkis is installed.
3. systemctl stop jenkins //cmd to stop the jenkins server
4. systemctl start jenkins
5. systemctl restart jenkins
6. ip addr show //command to show the ip of the jenkins


Architectural Over View of Jenkins:

Master:
1. Schedule Build Job.
2. Dispatch Builds to the slave for Actual Job Execution.
3. Monitoring the slave and recording the build results.

Slave:
1. Execute Build Jobs dispatched by master.

Jenkins Job: It refer’s to  runnable tasks thar are controlled and monitored by jenkins.
Eg: Suppose for your deployment on aws, gcp etc u can setup a job for that.
Eg: U want to build the project u can setup a job for that.
Eg: U want to execute some kind of sequel on the server u can set up a job for that.

Slave / Node: Slaves are the computer / vm’s that are setup to build projects for a Master.
	- Jenkins run seperate program called ‘Slave Agents’ (Java Program) on slaves.
	- When Slaves are registered to a master, a master starts distributing load to the slaves.
	- Node is used to refer all machines that are part of Jenkins grid, slaves and master.

Executor:  It is a seperate Stream of Builds to be run on a Node in parallel.
	Executor is like a thread it executes a single job on a slave. There can be multiple executors on a single node / slave.
- On a 4 GB RAM and 2 Processors cloud machine u can execute around 50 executors at a time. And it also depends on the task / type of work and the memory it takes.


Plugins: A plugin is like plugins on any other system, is a piece of software that extends the core functionality of the core Jenkins Server.



Continuous Integration with Jenkins

Jenkins Integration with GIT & GITHUB
- Install GitHub plugin on Jenkins Server using Mangae Plugins
- git --version

Install Maven on Jenkins Host Machine
- sudo apt-get install maven
- mvn –version

Configure Jenkins to Work with Maven  & GIT
Java Path
- /usr/lib/jvm/java-8-openjdk-amd64
Maven Path
- /usr/share/maven/

Git
- git //just git no path

Create Your First Maven Based Jenkins Job
Maven Goals: test, install, compile, package, deploy, test-compile etc

Source Code Polling In Jenkins
Continuous Integration flow is not for manual work. 

You have to select the Poll SCM

In Jenkins schedule is basically in CRON Basis.

Remote Build Trigger In Jenkins
- Remote Build Trigger is helpful to execute the build externally.
- If an action is triggered on Server B then a job is triggered on Server A then Remote Build Trigger is helpful.
- U can call the Build Trigger by some script, API or UI Button Click Event.

JENKINS_URL/job/First%20Sample%20Job/build?token=TOKEN_NAME  
JENKINS_URL(http://134.122.28.225:8080/)

Archive Artifact In Jenkins
IN Jenkins Server
1. Build Tab
2. Add post-build action
3. Post-Build Actions (Invoke Top Level Maven Targets)
4. Files to archive – Enter **/*.jar Here ** means all the path available in your workspace
5. **/* Will archive all artifacts.

Install & Configure Tomcat in Staging Environment

Tomcat Installation on Server

1. sudo apt install unzip wget
2. cd / tmp
3. wget https://mirrors.estointernet.in/apache/tomcat/tomcat-8/v8.5.51/bin/apache-tomcat-8.5.51.zip
4. unzip apache-tomcat-8.5.51.zip
5. sudo mkdir -p /opt/tomcat
6. sudo mv apache-tomcat-8.5.51 /opt/tomcat

Change tomcat server port

7. cd /opt/tomcat/
8. cd apache-tomcat-8.5.51/
9. cd conf
10. vi server.xml
11. / connector (it will search)
12. change 8080 to 9090 then use esc key and enter wq the press enter button.
13. Go to bin directory.
14. ls -la
15. sudo chmod +x * (Set the executable permission to the 	all scripts which are inside the current directory)
16. ./startup.sh
17. ./shutdown.sh
18. cd conf
18. vi tomcat-users.xml

Edit Rolls so that jenkins can use the credentials

19.roles = manager-script, admin-gui, password: tomcat, delete the end two lines
20. shutdown and restart tomcat to apply the changes.

Deploying Staging / QA Environment
1. New Item
2. Enter an Item Name ( Deploy_Application_Staging)
3. Free Style Project -> Ok Button
4. Description and select discart old builds and specify the numbers.
5. Sourcode Management - > git
6. Select Delete workspace before build starts in Build Environement.
7. Add timestamps to the console output in Build Env.
8. Add build step - > Select Invoke top-level maven targets
9. Goals - >clean package
10. Select Advanced and specify the path of pom.xml file.
11. Add post-build actions and select Archieve the artifacts
12. Enter **/*.war
13. Build Now

1. Go to Mangage Jenkins
2. Go to Manage Plugins
3. Select Available
4. Search for copy artifact
5. Install without restart
6. Install Deploy to container plugin
7. Create a new job Deploy_application
8. At Add build step select Copy artifacts from another project
9. Enter the project name of previous project as  Package_Application
10. Check the Stable Build Option.
11. Artifacts to copy -> **/*.war
12.  In Post build-actions select Deploy war/ear to container
13. war/ear -> **/*.war
14. Contextpath -> /
15. Add container -> select Tomcat 8.x
16. Add Credentials -> Jenkins Credentials
17. username and password -> tomcat & tomcat
18. Now for credentials tomcat
19. Server url -> http://134.122.28.225:9090
20. Click on save.

Make the jobs dependent on each other:
1. Go to Package Application
2. Selct Post Build-Actions
3. Add post-build action and select Build other projects
4. Type and select Deploy_Application_Staging_en
5. Select Tirgger only if the build is stable
6. In Poll SCM put cron syntax for every minute
7. Save

Once you save u get one more option inside your Package Application as Downstream Projects.

Build Pipeline Plugin
Install the Build Pipeline Plugin
1. Click on + icon
2. Enter View name
3. Select Build Pipeline View.
4. Enter some Build Pipeline View Title
5. Select Initial Job – Package_Application
6. Click on Ok

Deploy to Production
1. cp -r apache-tomcat-8.5.51 / apache-tomcat-8.5.51-prod/
2. cd  apache-tomcat-8.5.51-prod/conf
3. vi server.xml
4. Edit the port to 9091 from 9090 of previous tomcat configuration file
5. start the server in staging eve bin/startup.sh
6. to see logs enter the command ->tail -f apache-tomcat-8.5.51-prod/logs/catalina.out and the command directory should opt/tomcat/

1. Create a new item
2. Enter an item name (Deploy_Application_Prod_Env)
3. This will deploy Java Tomcat Application in Production.
4. Discard Code builds 5 and 5.
5. Delete Workspace before build starts and Add time stamps in Build Env.
6. In Build Select Add build step and select Copy artifact from other project.
7. Project Name -> Package Application
8. Select Stable Build.
9. Artificats to copy  -> **/*.war
10. Click on Post-Build-Actions and select Deploy war/ear to a container
11. Enter **/*.war
12. Context path -> /
13. Click Add Container and Choose the credentials tomcat.
14. Enter the tomcat url -> http://134.122.28.225:8081/
15. Click on save.


1. Go to  Deploy_Application_Staging Configuration file
2. Go to Post-build Actions
3. Click Add post-build actions and select Build other projects (Manual Step)
4. Enter the downstream project Deploy_Application_Prod_Env
5. Click on save.

Infrastructuer As Code: 
Coding the jenkins job is called as Infrastructure as code instead of manually configuring them.

Introduction to Jenkins Job DSL:
DSL stands for Domain Specific Language.
1. Go to Manage Plugins.
2. Search for Job DSL
3. Install Job DSL.


Demo: Jenkins Job DSL with Maven Project
1. Write the infrastructure code in MavenProjectDSL.groovy
For jenkins naming convention refer: http://jenkinsci.github.io/job-dsl-plugin/#path/job/

1. New Item
2. Item Name -> Maven_Seed_Job
3. Freestyle Project.
4. Click Ok Button.
5. Source Code Mgmt - > Selct Git.
6. Copy Paste the Repository Url of the DSL file.
7. In Build Options select Process Job DSL’s
8. Copy paste the DSL Script path in DSL Scripts field. (MavenProjectDSL.groovy )
9. Save.
10. Go for build now. U will get an error to approve the script.
11. Go to manage jenkins
12. Go to In-Process Script Approal.
13. Approve the script. And again build.

Demo Code Pipeline
Create a new view
1. View name (CodePipeline)
2. Select Pipeline View.
3. Ok

1. Create New Item
2. Enter name (SampleCodePipeline)
3. Select Pipeline
4. Ok
5. Description – This is the first and sample pipeline job.
6. U can copy and paste the pipeline script in th Pipleline section.  Or select Pipline script from SCM.
7. Select git and copy past the url of git project.
8. Define the file name in the script path (Jenkinsfile)
9. Save


Automate Existing Maven Project Pipeline
1. Inside Tomcat Application View
2. Create a new Item
3. Select Pipeline
4. Description( This is Code Pipeline Job to Package the Maven Project)
5. Select Discard Old Builds 5 5
6. In Pipeline select Pipelinescript from SCM
7. Select Git and copy paste the repository URL
8. Paste the path as java-tomcat-sample/Jenkinsfile for script path
9. Build Now.
10. We got an error as “The goal you specified requires a project to execute but there is no POM in this directory (/var/lib/jenkins/workspace/Package Application Code Pipeline). Please verify you invoked Maven from the correct directory.
“
11. Hence we need to define the complete path of pom.xml in the Jenkins file.
12. sh 'mvn -f java-tomcat-sample/pom.xml clean package'
13. Build Now.

Now configure the Deploy Application Staging and Prod Env jobs according to the Packaging Application Code Pipeline.
1. Go to Deploy_Application_Staging and click on configure.
2. Go to Build and change the existing project of Package Application to Package Application Code Pipeline
3. Delete the post build actions configuration of Deploy_Application_Prod_Env.
We will handle this in the Jenkinsfile.
4. Save the job.

Modify the same thing in the production environment.
1. Go to Deploy_Application_Production and click on configure.
2.  Go to Build and change the existing project of Package Application to Package Application Code Pipeline
3. Save the job.
4. Now modify the Jenkins file and add the starge to deploy the staging env.


Distributed Builds Concept
Here there will be on Jenkins Master and other slaves.

Create and Configure Jenkins Slave
1. Create the slave node.
2. Login to digital ocean.
3. Click on create button.
4. Select Droplets
5. Select Ubuntu
6. Select the bare mininum machine.
7. Select Newyork
8. Select OTP
9. Choose Name -> jenkinsslave1
10. Create droplet

IP Address: 198.211.100.174
Username: root
Password: Aistle7868@

Jenkins Master will use a specific user called as Jenkins User to ssh on the slave machine.
Hence we need to switch on the Jenkins user from the root user in master.
1. sudo -iu jenkins
We need to configure the ssh from master to slave with out the password. For this we will use the public private keys. We will generate a rsa certificate on my machine.
1. ssh-keygen -t rsa
2. Press enter twice.

The paths will be
/var/lib/jenkins/.ssh/id_rsa.
/var/lib/jenkins/.ssh/id_rsa.pub.

We need to create the .ssh directory on slave as well in order to configure master slave
1. ssh root@198.211.100.174 mkdir -p .ssh (On Master) as jenkins user.
2. Enter the password of slave machine
we need to copy the public key on master to the slave node on ssh directory.
3. cat /var/lib/jenkins/.ssh/id_rsa.pub | ssh root@198.211.100.174 ‘cat >> .ssh/authorized_keys’ (type it don’t copy paste it)
4. Enter the password of slave
5.  ssh root@198.211.100.174 (By using this command u can login to the slave from the master terminal) without password.
6. exit (to exit form slave)


On slave
1. mkdir -p /bin
2. mkdir bin
Inside the bin directory we need to download the slave jar for that we need to download it from the master
3. wget http://134.122.28.225:8080/jnlpJars/slave.jar

We need to install the java
4. sudo add-apt-repository ppa:webupd8team/java
5. sudo apt-get update
6. sudo apt install openjdk-8-jdk

Now we will go to the Jenkins and attach the slave to it.
1. Manage jenkins
2. Manage Nodes
3. New Node.
4. Node Name (JenkinsSlave1)
5. Select Permanent Agent
6. Description : Unix Client
7. No of executors: 2 executor on 1 core machine
8. Remote Root Directory: /usr/jenkins	( U can define any directory)
9. Labels : Linux
10. Launch Method -> Launch agent via execution of command on the master
11. Launch Command -> ssh root@198.211.100.174 java -jar /root/bin/slave.jar
12. Save


Concurrent Build Execution
1. New View (Concurrent Jobs)
2. Select List View
3. Ok
4. Click Ok
1. New Job
2. Concurrent Job 1
3. Free sty;e
4. Old build 5 5
5. Build step (Enter shell script)
6. Save

Similarly create two more jobs with same script Con Job2, Con Job 3

Execute all the jobs they will be executed accordtiong to the master schedule.


If you want to execute some specific job on some specific slave then then in order to achieve this 
1. We go to manage nodes.
2. Select the slave
3. Go to configuration
4. Lable (Linux)
5. Usage -> Only build jobs with label expression matching this node.

To assign a job to particular slave 
1. Go to configure the job.
2. Check Restrict where this project can be run.
3. Enter the label expression there.
4. Save


Available Docker Editions
Docker Editions
1. Communition Edition
2. Enterprise Editions

Community Edition
1. Edge Release
2. Stable Release

Docker Editions at store.docker.com

Install Docker on Linux Machine
1. docker version ( to see the docker version)
2. https://docs.docker.com/install/linux/docker-ce/ubuntu/
3. sudo apt-get update
4. sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common
5. curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
6. lsb_release -a (Command to check ubuntu version)
7. sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
8. sudo apt-get update
9. sudo apt-get install docker-ce
10. sudo usermod -a -G docker $USER (command to add root in docker sudo user)
11. sudo docker run hello-world


Docker Basics & Run Container
1. Images Vs Containers
2. Run Containers
3. Check container logs &  process

Why Containers required?
What is Image?
Image is a package of sourcecode , binaries, dependencies and execution environment.
Image is application we want to run.

With the help of docker we will create an executable image that contain the application binaries, sourcecode, dependencies of other applications and dependencies of the execution environment.

The process which is executing that particular image is called the containers.
Containers are required to execute the image.
Containers is an running instance of image.
Docker provides an execution environment to the container.
You can have many containers running the same image

Here we will use Open Source Nginx  Web-Server

Docker Central Repository (hub.docker.com)

Created Credentials for hub.docker.com

It is the central repository of all the docker images.
You can create a private / public account.

Docker commands:
1. docker run hello-world
2. docker container run hello-world

Search for nginx in hub.docker.com
3. docker pull nginx
4. docker info

Start Nginx web-server in Docker
docker container run --publish <host_port:container_port><image_name>
docker container run --publish 80:80 nginx (it will choose the random name)
docker run --name nginx -d -p 8080:80 nginx (it will put the name as nginx)

Process Done on above commands:
Downloaded images from Docker Hub.
Started new container
Exposed port 80 on Host Machine
Routes traffic to the container port 80

Start container in the backgroun or detach mode
docker container run –publish <host_port:container_port> --detach	<image_name>
docker container run --publish 80:80 --detach nginx
the above command will generate the container id
3e8f64f1ba328b63bb7b4cbdb8a1a2c750170612482e9d0b468574d30647f111
docker container run --publish 8081:80 --detach nginx
b4f5ecf71d37ea066f1463f548b9bf71c8286dc6919ddfcee08acd945cf887f9
You will be able to access nginx on both the ports 8080 and 8081and both are same host machine
This is the beauty of docker you can’t do this with the help of VM’s or other hosting applications.You can run the same application twice on same machine with the help of docker.

List Running Containers
docker container ls
docker ps (old way)

Docker File Basics | Introduction
Docker can build 	images automatically  by reading the instructions from a Docker File.
Docker File is a text document that contains all the commands a user could call on the command line to assemble an image.
A Docker images consists of read only layers each of which represents a Dockerfile instruction.
Command to build image from Dockerfile:
docker build -f <path_of_docker_file>

Docker File Instructions | Construction Commands
See sample docker file on hub.docker.com -> hello world, tomcat

1. FROM: The From instruction initializes a new build stage and sets the Base Image for subsequent instructions. It is first instruction.
A valid docker file must start from the From instruction
Base Image can be any valid image. (It can be linux, alpine etc..)	
Format
	FROM <image> [:<tag>]

2. LABEL: It is added to image to organise images by project record licensing information.
For each label, add a line begining with LABEL and with one or more key-value pairs.
Eg: LABEL com.example.version=”0.0.1-beta”
	LABEL vendor1=”ACME Incorporated”	

3. RUN: It will execute any commands in a new layer on top of current image and commits the results.
The resulting commit image will used for next step in the Dockerfile.
Eg: FROM ubuntu:14.04
	RUN apt-get update
	RUN apt-get install -y curl

4. CMD: Should be used to run the software contained by your image, along with any arguments.
Format: CMD [“executable”, “param1”, “param2”)]
There can be only on CMD instruction in the Dockerfile. If you list more than one CMD then only the last CMD will take effect.
The main purpose of a CMD is to provide defaults for an executing container.

5. EXPOSE: It indicated the ports on which a container listen for connections.
Format: EXPOSE	<port>

6. ENV: It sets the environment variable for you application.
<Key>to the value <value>
To make new software easier to run, you can use ENV to update the PATH environment variable for the software your container installs.

7. ADD: It copies new files, directories from remote file URL’s from <src> and adds them to the filesystem of the image path<dest>
Format: ADD hom* /mydir/ #adds files starting with hom

8. VOLUME: It should be used to expose any database storage area, configuration storage or files/folders created by your docker container.

9. WORKDIR: It setst the working directory for any RUN, CMD, ADD instructions that follow it in the Dockerfile.

Create Docker File of Out Project
1. Go to jenkins server in Tomcat Application and refer Package Application Code Pipeline
2. Copy & rename the java-tomcat-sample to java-tomcat-docker
3. Create a new Dockerfile in the project and write the commands ->see dockerfile.
4. Modify Jenkins file
5. Go to Jenkins Server and create a Docker Integration View (List View)
6. Create a new Job (Build_Tomcat_Docker_Image)
7. Select Pipleline Project
8. Ok
9. Description -> This project will create a Docker Image of java-sample-tomcat Project.
10. Discard Old Builds 5 5
11. Pipeline -> Pipeline Script from SCM
12. SCM -> Git.
13. Paste Repository URL
14. Save
15. Build Now

Result -> Error in docker image
16. Modify the docker file
17. Check in the code and Build now.
 
Build & Tag Project Docker Image
1. Create a list view with name (Docker Integration)
2. Create a new repository in the git and upload the maven-tomcat-sample-docker project.
3. Modify the paths in Dockerfile.
4. Go to Jenkins Server and create a new job (Build_Tomcat_Docker_Image)
5. Select Pipleline and click on Ok
6. Description > The project will create the Docker Image of Java-Sample-Tomcat-Project
7. Discard Old Builds 55
8. Pipeline -> Pipeline script from SCM
9. Copy paste the new repository url.
10. Save
11. Build Now.
12. Result -> Error
13. Change the docker file and execute.
14. docker images (in terminal to see the images)

If you got permission issues then
1. sudo usermod -a -G docker jenkins
2. systemctl restart jenkins

Execute Project in Docker
1. docker images
2. docker --version
3. docker container run -p 9090:8080 tomcatsamplewebapp:3
4. http://134.122.28.225:9095/java-tomcat-maven-example/
The Colored is the war file name

Add Parameters in Jenkins Job
1. Create a new view (Parameterized Job)
2. Select Listview
3. Ok

Parmeterized job requires dynamic parameters at run time.

1. Create a new item.
2. First_Parameterized_Job.
3. Free style project.
4.  Thisi s my first parameterized jenkins job.
5. Disard Old builds 5 5
6. Build -> Execute shell script
	echo “This is my first parameterized job”
	echo “My name is mohammed sibgathulla”
7. save
8. Build now.

1. Configure
2. General -> Check this project is parameterized.
3. Add parameter -> String paramenter.
4. Name -> First_Name
5. Default Value -> Mohamemd Sibgathulla.
6. Alter echo “My name is $First_Name”
7. Save.
8. Build with parameters

Note: If you require more parameters then it may lead to high maintenance cost.

Add Choice Parametrs in Jenkins Job
1. Configure
2. Add Parameter -> Select Choice Parameter.
3. Enter name and choices
4. Save.
5. Build with paramerters.

Add Logic with Boolean Parameter
1. Configure
2. Add Parameter -> Select Boolean Parameter.
3. Enter name and Select Default value
4. Replace the name in script and compare it.
5. Save and build.

Enable / Disable Login Jenkins
To Disable login
1. Go to manage jenkins
2. Configure Global Security
3. Uncheck Enable Security
4. Save
5. The server will be open for anyone.

To Enable
1. Go to manage jenkins
2. Configure Global Security
3. Check Enable Security
4. Select Jenkin’s own user database
5. Un Select Anyone can do anything
6. Select Logged In user can do anything.
7. Uncheck Allow anonymous read access.
8. Save.

Allow User to Sign-Up Jenkins
1. Manage Jenkins
2. Configure Global Security
3. Select Allow User to Signup
4. Save
The test user has all the credentinal that a super use has it has flaws.

Install Powerful Security Plugin
1. Manage Jenkins
2. Manage Plugins
3. Search for Role based Authorized
4. Install and restart. And check for installation.
5. Manage Jenkins
6. Configure Global Security
7. Check Role-based Strategy
8. Save. And scroll u will get new options Manage & Assign Roles.
9. You can assign different roles by using this setting.

How to create Users in Jenkins
1. Go to manage jenkins
2. Configure Global Security
3. Manage Users.
4. Create User
5. Fill the details.
6. After u logged in you will not have any permissions.

Create & Assign Roles to Users
1. Go to manage jenkins
2. Manage & Assign Roles
3. Manage Roles
4. Role to Add -> read-only
5. Select Read.
6. Save
7. Assign Roles
8. User/Group to Add -> testuser
9. Select read-only
10. Save
Now refresh the previous browser. Now the test user can see the read-only things.
Similarly you can give many accesses to the user.

Check Docker Install & Configuration
- First we will learn about containers.
- Check versions of Docker CLI
- Learn Container Management Commands
- Learn Docker Networking Basics.
Commands
1. docker version
2. hostname -> Will return the hostname of the machine
3. docker --version
4. docker info
5. docker –info
6. docker (it will list out all the commands);
Docker Management commands format
docker <command> <sub-command> (optional)
Eg: docker container run

Start Your First Container
Image vs Container
run containers
check container logs and process


Stop-Remove the Containers 
1. docker container stop <container_id>
2. docker container ls (To see containers which are running)
3. docker container ls -a (To see all container that are active and inactive)
4. docker container run (start a new container always)
5. docker container start (start a existing container)
6. docker container run --publish 80:80 --detach --name<Name> <Image_Name> (Docker Container Names)
7. docker container logs <container_name> / <container_id> (Seel Logs of a Specific Container)
8. docker container top <conainer_id> (See Process Running Inside The Container)
9. docker container rm <space seperated container ids> (Remove All Unused Containers)
10. docker container rm -f container_id (Forcefully remove a running container)

Docker Internal Processing
Docker runs on port 80 on host machine and routes all the traffic to port 80 inside the container.

Container vs Virtual Machines
1. ps aux (To see the processes running on host machine)
2. ps aux | grep nginx

Assignment : Manage Multiple Containers
Assignment: Answers Manage Multiple Containers
1. docker container run -d -p 80:80 --name proxyserver nginx
2. docker container run -d -p 8080:80 --name webserver httpd
3. docker container run -d -p 3306:3306 --name mysqldb --env MYSQL_RANDOM_ROOT_PASSWORD=yes mysql
4. docker container logs mysqldb

Docker CLI Monitoring
1. docker container run -d --name proxyserver nginx
2. docker container run -d --name mysqldb --env MYSQL_RANDOM_ROOT_PASSWORD=true mysql
3. docker container inspect container name /id
4. docker container stats container name / id

Start Container In Interactive Mode
1. docker container run -it //Run Container Interactivity.
2. docker container run --help
3. docker container run -it --name webproxy nginx bash
4. ls -lrt
5. exit //To exit from bash of container

Run Commands In Running Containers
1. docker container exec -it
2. docker exec
3. docker container ps / ls 
4. docker container run -it  --name nginxweb nginx bash
5. docker container exec -it 1b3703cc2343 touch /tmp/mohammed
6. docker container exec -it 1b3703cc2343 bash
7. docker container run -it ubuntu bash
8. apt-get update // the curl command to open facebook won't works as the docke ubuntu has less feature hence we need to update
9. apt-get install curl
10. curl https://www.facebook.com

Docker Network Introduction
1. See the pdf file.

Explore Container Networks
1. docker container run -p 8080:80 -d nginx
2. ifconfig eth0 //to find the ip of local machine
3. docker container port container_id
4. docker container inspect container_id  // see the network settings and check ip address 
5. docker contianer inspect --help
6. docker container inspect -f '{{.NetworkSettings.IPAddress}}' container_id
//the container will not use the local network it will have its own virtual private network and its called 'bridge'
7. docker container stop container_id
8. docker container start container_id

Docker Network CLI : List & Inspect
1. docker network ls //list all the networks on host machine
2. docker network ls -f driver=bridge
3. docker network --help
4. docker network ls --format "{{.ID}}: {{.Driver}}"
5. docker network inspect container_id

Docker Network: Create, Connect & Disconnect
1. docker network create mynetwork // By default the driver is bridge.
2. docker network inspect mynetwork
3. docker container run -d --name my_nginx --network mynetwork nginx //connecting the container with mynetwork
4. docker network connect mynetwork container_name // to connect existing container to a network
5. docker network disconnect network_name container_name //If you disconnect all the network from a container then you will not be able to communicate with the container

Docker Network: DNS Concept
- Docker uses DNS to communicate and it doesn't use IP Address.
1. docker container run -d --name my_nginx1 --network mynetwork  ngnix:alpine
2. docker container run -d --name my_nginx2 --network mynetwork  ngnix:alpine
3. docker container exec -it my_nginx1 ping my_nginx2 // here containers are using name to communicate with each other the ip may assigned to other names hence we use dns

What Is Docker Image
1. docker images //to see all docker images.
2. docker ps //to see any docker image running

Docker Hub Repository
- In Explore link you will get all the official docker images.
1. docker pull <image_name>:<version>
2. docker pull <image_name> //for latest

Concept Of Docker Image Layers
1. docker history <image_name> //to see the different layers of the image
2. docker inspect mysql //to see the complete metadata of the image

Docker Images Tagging
1. docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]
Eg: docker tag ubuntu ubuntutest
    docker tag mysql:5.7 mysqlmohammed:test //if you don't provide tags then we have to use the latest hence you must provide tags 

Upload Docker Image To Cloud
1. docker login
2. docker image push USER/Image-name //mohammed/imagename or the image will not be pushed so u should create tag with username for the image and then push it.
3. docker logout

Dockerfile Basics
1. docker build -f <path_of_docker_file>

Dockerfile Instructions
Refer the pdf file.

Create Custom Docker Image
1. docker image build -t customnginx:0.01 /root/
2. docker run -d -p 4444:90 customnginx:0.01 //the port number 90 should be same in the docker file
3. scp filename root@ipaddress:/root //command to push a file from local to remote server

Extended Official Docker Image
In the docker file we have not used CMD as bcoz it is already present inside he official nginx image that we haved written as base images.
1. docker image build -t nginx-customer:0.0.1 /root/ 
2. docker container run -p 8080:80 nginx-custom:0.0.1

Assignment Build Docker Image
See the pdf file.

Assignment Answer Build Docker Image
1. docker image build -t python_prog:0.0.1 /root/
2. docker container run python_prog:0.0.1
3. docker imgate rm -f python_prog:0.0.1 python:latest
4. docker image build -t mohammedhshaik81/python_prog:0.0.1 /root/
5. docker login //Enter the credentinals
6. docker push mohammedshaik81/python_prog:0.0.1
7. docker image rm mohammedshaik81/python_prog:0.0.1
8. docker container run mohammedshaik81/python_prog:0.0.1 // this image is not present in local it will download from your docker account

Persistent Data Problem
- We can solve the persistent data problem of containers with the help of Data Volumes & Bind Mounts.

Persistent Data Volumnes
1. docker pull mysql
2. docker inspect mysql //check for volumes.
3. docker run -d --name msyqldb -e MYSQL_ALLOW_EMPTY_PASSWORD=True mysql  
4. docker ps
5. docker inspect mysqldb //check for volumes & mounts and u can go to that particular location and see the data.
6. docker volume ls
7. docker run -d --name msyqldb2 -e MYSQL_ALLOW_EMPTY_PASSWORD-True mysql  
8. docker volume ls
9. docker container stop mysqldb 
10. docker volume ls //the containers are destroyed but the data is present.
11. docker volume inspect volume_id
12. docker run -d --name msyqldb3 -e MYSQL_ALLOW_EMPTY_PASSWORD=True --mount source=mysql-db,destination=/var/lib/mysql mysql  
13. docker volume ls // a new volume with custom name mysql-db will create for the above command as we have more than one volume we cannot identify with id so we use above custom names
14. docker container stop mysqldb3
15. docker run -d --name msyqldb4 -e MYSQL_ALLOW_EMPTY_PASSWORD=True --mount source=mysql-db,destination=/var/lib/mysql mysql  //started mysqldb4 with same volume of mysql-db

Persistent Data: Bind Mounts
1. mkdir dockerbind //at root directory
2. docker container run -d --name nginxbind --mount type=bind,source=$(pwd),target=/app nginx
3. docker ps
4. docker exec -it nginxbind bash 
5. echo "hi this is custom file" > test.txt //in host terminal
6. touch mohammed.txt

Assignment: Data Volumes
- See pdf file.

Assignment Answer: Data Volumes
1. docker ps -a //to verify how many containers are running / exists on the machine
2. docker container run --name=mysql-test mysql:8.0 //Will generate an error database is uninitialized and password option not specified.
3. docker container run --name mysql-test -e "MYSQL_ROOT_PASSWORD=mypassword" mysql:8.0
4. docker container run -d --name mysql-test -e "MYSQL_ROOT_PASSWORD=mypassword" --mount source=mysql-db,target=/var/lib/mysql mysql:8.0
5. docker inspect mysql-test
6. mysql -u root -p mypassword -h ipaddress -P 3306
7. apt-get install mysql-client
8. mysql -u root -pmypassword -h ipaddress -P 3306
9. create database empdb;
10. show databases;
11. use empdb;
12. CREATE TABLE Employee ( EmployeeID int, LastName varchar(255), FirstName varchar(255), Address varchar(255), City varchar(255));
13. INSERT INTO Employee (EmployeeID, LastName, FirstName, Address,City)VALUES (14, 'B. Erichsen', 'Tom', 'Skagen 216', 'Norway');
14. INSERT INTO Employee (EmployeeID, LastName, FirstName, Address,City)VALUES (17, 'Zbyszek', 'Wolski', 'Keskuskatu 45', 'Finland');
15. commit;
16. select * from employee;
17. exit;
18. docker container stop mysql-test;
19. docker ps;
20. docker container rm mysql-test;
21. docker container run -d --name mysql-test-latest -e "MYSQL_ROOT_PASSWORD=mypassword" --mount source=mysql-db,target=/var/lib/mysql mysql:8.0.19
22. mysql -u root -p mypassword -h ipaddress -P 3306
23. show databases;
24. use empdb;
25. select * from employee;

Assignment: Bind Mounts
- See pdf file.

Assignment Answer: Bind Mount
1. docker ps -a 
2. ifconfig
3. docker container run -d --name nginx-test -p 80:80 nginx //ipaddress:80/index.html type in browser
4. docker container stop nginx-test 
5. docker exec -it nginx-test bash 
6. cd /usr/share/nginx/html and type ls // you can see index.html file
7. mkdir testnginx
8. docker container run -d --name nginx-bind -p 80:80 --mount type=bind,source="$(pwd)",target=/usr/share/nginx/html nginx
9. docker exec -it nginx-bind bash
10. cd /usr/share/nginx/html //there will be no file present

Docker Compose Introduction
1. docker version
2. sudo curl -L "https://github.com/docker/compose/releases/download/1.25.4/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
3. sudo chmod +x /usr/local/bin/docker-compose
4. docker-compose --version

Docker Compose YML File
- See pdf file.

Run MySql & Wordpress via Docker Compose
1. touch docker-compose.yml
2. chmod 777 docker-compose.yml //providing the executable permission.
3. docker-compose up -d
4. docker contianer logs container_id
5. docker container logs -f container_id //this will show the logs in running mode

Docker Compose: Build Application From Scratch
1. docker-compose -f custom-application.yml up -d
2. docker-compose -f custom-application.yml down
3. docker volume create --name=data

Docker Swarm Introduction: Swarm Orchestration
- See pdf file.

Docker Swarm Terminology
- See pdf file.

Create Service On Docker Swarm
1. docker info
2. docker swarm init
3. ifconfig -a
4. docker swarm init --advertise-addr ipaddress //it is used when we have more than one ip we use the eth0 ip by using the previous command
5. docker swarm --help
6. docker service --help
7. docker service create alpine ping www.google.com //this will create a linux and image and run the service by pinging google.com on linux
8. docker service inspect clever_kalam (servicename)
9. docker service ls
10. docker service ps clever_kalam //identifying running container for the service
11. docker container inspect container_id 
12. docker service ls //See replicas
13. docker service update clever_kalam --replicas 4 //scale up the services
14. docker container rm -f container_id  //if you stop a service the docker swarm will initiate the new service as we have defined 4 replicas. the docker orchestration system will completely handle configuration / orchestration of your docker service

Create Docker Swarm Cluster
1. sudo apt-get update
2. sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common
3. curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
4. sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
5. sudo apt-get update
6. sudo apt-get install docker-ce
7. sudo usermod -a -G docker root

//commands to install docker machine
8. base=https://github.com/docker/machine/releases/download/v0.14.0 && curl -L $base/docker-machine-$(uname -s)-$(uname -m) >/tmp/docker-machine 
9. sudo install /tmp/docker-machine /usr/local/bin/docker-machine
10. docker-machine version

//commands to install docker compose
11. sudo curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
12. sudo chmod +x /usr/local/bin/docker-compose
13. docker-compose --version

//commands to create docker swarm cluster
14. docker swarm init --advertise-addr ipaddress //init on all the nodes
15. docker node --help
16. docker swarm join-token manager //copy paste the result in all other nodes.
17. docker node ls
18. docker swarm leave -f //execute if u get the error message that it is already part of a swarm.
19. docker service create --replicas 8 alpine ping www.google.com //executed on master
20. docker service ls
21. docker service ps service_name //it will show which service is running on which machine
22. docker container ls //on worker nodes and master. 

References to Setup Docker on Swarm Docker Nodes
1. Reference : https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-docker-ce-1
2. Reference : https://docs.docker.com/machine/install-machine/#install-machine-directly
3. Reference : https://docs.docker.com/compose/install/#install-compose

Networks in Docker Swarm
1. docker network ls
2. docker network create -d overlay my_overlay //to create a custom user defined network.
3. docker service create --name postgres --network my_overlay -e POSTGRES_PASSWORD=mypassword postgres
4. docker service create --name mydrupal --network my_overlay -p 80:80 drupal
5. docker service ls
6. docker service ps postgress
7. docker service ps drupal

Docker Swarm Traffic Management
1. docker service inspect --format="{{json .Endpoint.Spec.Ports}}" mydrupal

Assignment: Deploy Multi-Node Application In Docker Swarm
- See pdf file.

Assignment Answer: Deploy Multi-Node Application In Docker Swarm
1. docker network create -d overlay front_end_ntw
2. docker network create -d overlay back_end_ntw
3. docker service create --name vote -p 5000:80  --network front_end_ntw --replicas 5 dockersamples/examplevotingapp_vote:before
4. docker service create --name redis --network front_end_ntw --replicas 5 redis:3.2
5. docker service create --name worker --network front_end_ntw --network back_end_ntw dockersamples/examplevotingapp_worker:latest
6. docker service logs worker
7. docker service create --name db --network back_end_ntw --mount type=volume,source=db-data,target=/var/lib/postgresql/data postgres:9.4
8. docker service create --name result --network back_end_ntw -p 5001:80 dockersamples/examplevotingapp_result:before

Docker Swarm Stacks
1. docker build --tag=friendly_hello:v1.0.1 . //the . tells to take the resources(dockerfile) from the current directory.
2. docker login
3. docker tag friendly_hello:v1.0.1 mohammedshaik81/frindly_hello
4. docker push mohammedshaik81/frindly_hello
5. docker stack --help
6. docker stack deploy -c docker-compose.yml nginx_start 
7. docker stack ls
8. docker service ps nginx_start_web // to see the task running on different containers
9. docker stack services nginx_start
10. scp -r docker_project root@167.172.140.79:/root //command to push a directory to server using scp

Docker Swarm Stack: Scale Application
- Edit the yml file and change the replicas, cpu, memory etc.
1. docker stack deploy -c docker-compose.yml nginx_start 
2. docker service ps nginx_start_web
3. docker service ps ngix_start_visualizer

Docker Swarm Persistent Data Issue In Distributed Application
1. docker stack deploy -c docker-compose.yml nginx_start //here we can write custom name instead of nginx_start
2. docker service ps nginx_start_redis
3. docker service rm nginx_stop_redis
4. docker stack deploy -c docker-compose.yml nginx_start //again to start the redis

Swarm: Deploy Distributed Application
1. docker stack deploy -c docker-stack.yml votingstack
2. docker stack ls
3. docker stack ps votingstack
4. docker container ls
5. docker stack services votingstack

Docker Swarm Secrets Introduction
- See pdf file.

Create Docker Service With Secrets
1. docker info //see for swarm active and Raft
2. docker sercret --help
3. mkdir secrets_example
4. vi dbpassword.txt, then enter the password and save the file.
5. docker secret create db_password dbpassword.txt
6. echo "db_user" | docker secret create db_username -
7. docker secret ls
8. docker secret inspect db_password
9. history
10. docker service create  --name postgress --secret db_username --secret db_password -e POSTGRES_PASSWORD_FILE=/run/secrets/db_password -e POSTGRES_USER_FILE=/run/secrets/db_username postgres
11. docker service ps postgress //we can see the container name of the running service.
11. docker exec -it postgress.1.1fqasdflasdfk bash
12. cd /run/secrets/

Deploy Stack With Swarm Secrets
1. docker stack deploy -c docker-compose.yml postgredb
2. docker stack deploy -c docker-compose.yml posgres_os_stack
3. docker secret ls
4. echo "mytestvalue" | docker secret create my-secret -
5. docker secret ls

Zero Downtime Service Upgrade
1. docker service create -p 80:8080 --name  web_server nginx:1.14.2
2. docker service scale web_server=10 //scalling horizontally
3. docker service ls
4. docker service ps web_server
5. docker service update --image nginx:1.15.12 web_server //this is called as rolling update.
6. docker service ps web_server
7. docker service update --publish-rm 8080 --publish-add 9090:80 web_server //to republish the port

Health Check In Docker Swarm
1. mkdir healthcheck
2. docker container run --name postgres1 -d -e POSTGRES_PASSWORD=mypassword postgres
3. docker container exec -it postgres1 bash
4. pg_isready -U postgres //it will show whether postgres is accepting connections or not.
5. docker container run -d --name postgres2 --health-cmd="pg_isready -U postgres || exit 1" -e POSTGRES_PASSWORD=mypassword postgres
6. docker ps
8. docker container run -d --name postgres3 --health-cmd="pg_isready -U root || exit 1" -e POSTGRES_PASSWORD=mypassword postgres  //instead of the postgres user we are trying to access the root user.
9. docker ps
10. docker service create --name postgresservice1 -e POSTGRES_PASSWORD=mypassword postgres
11. docker sevice ls
12. docker service create --name postgresservice2 --health-cmd="pg_isready -U  || exit 1" -e POSTGRES_PASSWORD=mypassword postgres //here it will wait for 30 sec as the startup time 

Container Placement In Docker Swarm
1. docker run -it -d -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock dockersamples/visualizer
2. docker container ls
3. docker node ls
4. docker service create --name postgresdb --constraint node.role==manager -e POSTGRES_PASSWORD=mypassword postgres
5. docker service create --name mynginx --constraint node.role==worker --replicas=5 nginx
6. docker node ls
7. docker node update --label-add=region=east-1-d node_id
8. docker node inspect node_id //in order to see the lable created.
9. docker service create --name postgress --constraint node.labels.region==east-1-d -e POSTGRES_PASSWORD=mypassword postgres
10. docker service update --constraint-rm=node.labels.region==east-1-d --constraint-add node.role==worker postgresdb

Service Constraints In YML File
1. docker stack deploy -c docker-compose.yml mysql
- See the yml file.

Kubernetes Introduction & Uses
- See the pdf file.

Kubernetes Architecture
- See the pdf file.

Orchestration Giants: Kubernetes vs Docker System
- See the pdf file.

Kubernetes Setup
- See the pdf file.

Setup Kubernetes With Mini Kube
1. Create a droplet.
2. df -k //to see the space on the droplet.
3. df -h //to see space

//Install docker on linux machine 
1. sudo apt-get update
2. sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common
3. curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
4. sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
5. sudo apt-get update
6. sudo apt-get install docker-ce

//Install KubeCtl
1. curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
2. chmod +x ./kubectl
3. sudo mv ./kubectl /usr/local/bin/kubectl
4. kubectl version --client


//Install MiniKube
1. curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube
2. sudo mkdir -p /usr/local/bin/
3. sudo install minikube /usr/local/bin   
    
//Install Virtual Box //We are installing virtual box because minikube will execute the kubernetes inside the vm, it will create a seperate vm and install the kubernetes
1. wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add -
2. sudo apt-get update
3. sudo apt-get install virtualbox 

//Execute MiniKube & Create Cluster
1. export CHANGE_MINIKUBE_NONE_USER=true
2. sudo apt install conntrack
3. sudo -E minikube start --vm-driver=none //MiniKube will create a vm inside your box that vm will work as a worker and your node will work as a master.
4. echo ‘export CHANGE_MINIKUBE_NONE_USER=true’ >> ~/.bashrc
5. kubectl cluster-info

//Interact Cluster Using KubeCtl
- Let’s create a Kubernetes Deployment using an existing image named
echoserver, which is a simple HTTP server and expose it on port 8080
using --port.
1. kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.10

- In order to access the hello-minikube service, we must first expose
the deployment to an external IP via the command:
2. kubectl expose deployment hello-minikube --type=NodePort --port=8080

- We can inspect the pods and the deployments
3. kubectl get pod
4. kubectl get deployments

- Check if the service was exposed
5. kubectl get services

- Get the URL of the exposed Service to view the Service details:
6. minikube service hello-minikube --url
    
- Now we can either curl the service from the CLI, or hit it via the browser.
7. curl $(minikube service hello-minikube --url)
8. curl <URL>

- Delete the Service
9. kubectl delete services hello-minikube
10. kubectl delete deployment hello-minikube

- Stop & Delete the local Minikube cluster:
11. minikube stop
12. minikube delete

Kops Introduction
- See pdf file.

Prepare AWS Environment For Kops
1. curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
2. chmod +x ./kops
3. sudo mv ./kops /usr/local/bin/

//Install Kubectl Commands
1. curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
2. chmod +x ./kubectl
3. sudo mv ./kubectl /usr/local/bin/kubectl

//Install Python Pip
1. sudo apt-get install python-pip
2. python --version

//Install AWS Cli
1. pip install awscli
2. aws --help
3. pip uninstall awscli
4. aws configure
5. Enter Access Key ID & Password from Amazon IAM User.
6. ls -lrt ~/.aws/ //to check the configuration
7. cat ~/.aws/credentials //this will show the credentials
//create a s3 bucke on aws console and add ur domain on Route 53 in Networking & Content Deloiver on aws console.

Kubernetes Setup On AWS Cloud
1. ssh-keygen -f .ssh/id_rsa //hit enter twice after this
2. cat .ssh/id_rsa.pub
3. kops create cluster --yes --state=s3://kops-storage-b34598 --zones=ap-south-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --name=test.k8s.local --ssh-public-key ~/.ssh/id_rsa.pub

//search for aws availability zones in google add a to d to zone like ap-south-1a

4. kops validate cluster --state=s3://kops-storage-b34598
5. kubectl get node
6. kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.10
7. kubectl expose deployment hello-minikube --type=NodePort --port=8080
8. kubectl get services
//here u will get the port number u have to go to aws console and select any node and in security groups add a custom rule in inbound with the port number and allow for everyone.
9. kops delete cluster --name test.k8s.local --yes --state=s3://kops-storage-b34598


Build & Push Docker Custom Image
1. create a index.html file and dockerfile with content (From nginx:alpine /n . /usr/share/nginx/html) on root user.
2. docker build -t nagicalnginx . //the dot specifies the current directory
3. docker run -d --name alpinenginx -p 8081:80 nginx:alpine
4. docker run -d --name customnginx -p 8085:80 magicalnginx
5. docker tag container_id mohammedshaik81/magicalnginx
6. docker login --username=mohammedshaik81
7. docker push mohammedshaik81/imagename

Run First Custom Image On Local Kubernetes
1. minikube start
2. kubectl get node
3. kubectl get --help
4. kubectl create deployment magicalnginx --image=mohammedshaik81/magicalnginx //the image is coming from hub.docker.com and my account
5. kubectl get deployments
6. kubectl describe deployment deployment_name  
7. kubectl create service nodeport magicalnginx --tcp=80:80
8. kubectl get svc
9. minikube service magicalnginx --url
10. curl url
11. kubectl delete service service_name
12. kubectl delete deployment deployment_name

Run Custom Image On Kubernetes
1. kops create cluster --yes --state=s3://kops-storage-b34598 --zones=ap-south-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --name=fabulouzfashion.com --ssh-public-key ~/.ssh/id_rsa.pub
2. kops validate cluster --state=s3://kops-storage-b34598
1. kops validate cluster --state=s3://kops-storage-b34598 -o json
2. kops validate cluster --state=s3://kops-storage-b34598 -o yaml
3. kubectl create deployment magicalnginx --image=mohammedshaik81/magicalnginx
4. kubectl get deployments
5. kubectl describe deployment deployment_name  
6. kubectl create service loadbalancer magicalnginx --tcp=80:80
7. kubectl get services
11. kubectl delete service service_name
12. kubectl delete deployment deployment_name
13. kops delete cluster --name fabulouzfashion.com --yes --state=s3://kops-storage-b34598

Setup Kubernetes On GCP Cloud
# Add the Cloud SDK distribution URI as a package source
1. echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
2. apt-get install apt-transport-https ca-certificates
# Import the Google Cloud Platform public key
3. curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
# Update the package list and install the Cloud SDK
4. sudo apt-get update && sudo apt-get install google-cloud-sdk
#Docker tip: If installing the Cloud SDK inside a Docker image, use a single RUN step instead:
5. RUN echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y
#Install Additional Packages
6. sudo apt-get install google-cloud-sdk-app-engine-python
7. sudo apt-get install google-cloud-sdk-app-engine-python-extras
8. sudo apt-get install google-cloud-sdk-app-engine-java
9. sudo apt-get install google-cloud-sdk-app-engine-go
10. sudo apt-get install google-cloud-sdk-datalab
11. sudo apt-get install google-cloud-sdk-datastore-emulator
12. sudo apt-get install google-cloud-sdk-pubsub-emulator
13. sudo apt-get install google-cloud-sdk-cbt
14. sudo apt-get install google-cloud-sdk-cloud-build-local
15. sudo apt-get install google-cloud-sdk-bigtable-emulator
16. sudo apt-get install kubectl
#Initialize GCloud CLI:
17. gcloud init //User the link provided and copy paste and it will authenticate.
#Set Google Project to Work On:
18. gcloud config set project test-1512061290775
#List All the Installed Components
19. gcloud components list
#Set Deployment/Compute Engine Zone, Type google cloud zones and search for the best zone available for you.
20. gcloud config set compute/zone asia-south1-c //u can change it to b also
#Enable Few APIs on Your Project //search for api & services in google cloud console
#Click on Enable API's & Services and install all the below API's on Google Cloud
#Search for the below API's
21. Compute Engine API
22. Kubernetes Engine API
23. Google Container Registry API

Run Custom Image On GCP Kubernetes
1. gcloud container clusters create mykubernetes-cluster
2. gcloud container clusters describe mykubernetes-cluster
3. gcloud container clusters list 
4. kubectl create deployment magicalnginx --image=anshuldevops/magicalnginx
5. kubectl get deployments
6. kubectl describe deployment magicalnginx
7. kubectl create service loadbalancer magicalnginx --tcp=80:80
8. kubectl get svc
9. kubectl delete services magicalnginx
10. kubectl delete deployment magicalnginx
11. gcloud container clusters delete mykubernetes-cluster

Kubernetes Node Workflow
#A node can have n no. of pods and a pod can have n no. of containers and better if they are tightly coupled (nod mandatory) basically serving as a application, all this pods are managed by Docker.
#A kubelet will manage the communication between master and worker node. Each command fired by the master first comes to the kubelet and issue the direction to the worker node.
#A kubeproxy will manage the internal traffic of the worker node. It will maage the ip's being used by the containers and pods and these will be managed inside the rule table called iptables inside   the linux system. iptables follow the mapping between the pods, containers and load balancer.
- see the pdf file.

Scaling Pods In Kubernetes
- See the pdf file.

Lab: Scaling Pods In Kubernetes With Replication Controller
1. chmod 777 replication.yml
2. kubectl create -f replication.yml
2. kubectl get pods
3. kubectl get rc //to get the names of replication controller
4. kubectl describe rc/nginx(replication controller name)
5. kubectl describe pod pod_name
6. kubectl delete pod pod_name
7. kubectl scale --replicas=10 rc/rc_name
8. kubectl get pods
9. kubectl scale --replicas=2 rc/rc_name
10. kubectl delete rc/rc_name

Replica Set In Kubernetes
1. chmod 777 replicaset.yml
2. kubectl create -f replicaset.yml
3. kubectl get rs
4. kubectl get pods
5. kubectl describe rs/replicaset_name
6. chmod 777 pods.yml
7. kubectl create -f pods.yml //By this cmd the pods will be created and delted as already we haved defiend 3 pods in replicaset.yml file and we are using the fronend in tier of pods.yml file
8. kubectl get pods
9. kubectl delete rs/frontend 
10. kubectl create -f pods.yml //it will create 2 pods
11. kubectl create -f replicaset.yml //it will create only one pods as alread 2 are running with same name as frontend this is called known template base pods aquisition of the replica

Deployment In Kubernetes
1. See the pdf file.

Lab: Deployment In Kubernetes
1. chmod 777 webservicedeployment.yml
2. kubectl create -f webservicedeployment.yml
3. kubectl get deployments
4. kubectl get rs
5. kubectl get pods
6. kubectl describe deployment/deployment_name
7. kubectl get pods
8. kubectl get pods --show-labels
9. kubectl expose deployment deployment_name --type=NodePort
10. kubectl get service
11. minikube service service_name --url
12. curl url
13. kubectl set image deployment deployment_name nginx=mohammedshaik81/magicalnginx
14. kubectl rollout status deployment deployment_name
15. kubectl rollout history deployment deployment_name
16. kubectl get service
17. minikube service service_name --url
18. curl url
19. kubectl rollout undo deployment deployment_name
20. kubectl rollout status deployment deployment_name
21. curl url
22. kubectl rollout history deplooyment deployment_name
23. kubectl edit deployment deployment_name
24. kubectl rollout status deployment deployment_name
25. kubectl get pods

Services In Kubernetes
1. See pdf file of before commit.

Lab: Services In Kubernetes
1. chmod 777 podmanifest.yml
2. kubectl create -f podmanifest.yml
3. kubectl get pods
4. chmod 777 podservice.yml
5. kubectl create -f podservice.yml
6. minikube service nginx-service --url
7. curl url
8. kubectl delete service nginx-service
9. kubectl describe service nginx-service

Labels In Kubernetes
1. See pdf file.

Lab: Labels In Kubernetes
1. kubectl get nodes
2. kubectl get nodes --show-labels # It will show all the labels attached with the nodes
3. kubectl create -f podlabels.yml
4. kubectl get pods
5. kubectl describe pod pod-name
6. kubectl get nodes
7. kubectl label nodes minikube(it may be ur system name) disktype=ssd #It will add a lable with disktype: ssd to the existing node
8. kubectl get nodes --show-labels
9. kubectl get pods #Once u run the command the pending pod status will be Running as we have added the label disktype: ssd

Liveness(HealthCheck) Of Application In Kubernetes
1. See pdf file.

Demo Liveness(HealthCheck) Of Application In Kubernetes
1. chmod 777 podliveness.yml
2. kubectl create -f podliveness.yml
3. kubectl describe pod liveness-exec //after 60 seconds the conainer will fail and restart
4. kubectl get pods //you can see the restart count increases after every 60 seconds
5. chmod 777 podlivenesshttp.yml
6. kubectl create -f podlivenesshttp.yml
7. kubectl describe pod liveness-http
8. kubectl get pods

Readiness In Kubernetes & Application
- After the container is live it will start serving the traffic but the application may not be loaded due to which we will have downtime.
In some cases the container will be live in few second but the application take around 40 to 50 minutes to get up inside the container.
Hence in these cases we will go for Readiness probe. In some cases we don't need the Readiness & Liveness Probe as well u can check the pdf file for detail.
1. chmod 777 podreadiness.yml
2. kubectl create -f podrediness.yml
3. kubectl get pods

Lifecycle Of Pods In Kubernetes
1. chmod 777 podlifecyclehooks.yml
2. kubectl create -f podlifecyclehookds.yml
3. kubectl describe pod lifecycle-demo 
4. kubectl exec -it lifecycle-demo -- /bin/bash
5. cat /usr/share/message

Secretes In Kubernetes
1. kubectl create secret generic db-cred --from-file=./username.txt --from-file=./password.txt
2. kubectl get secrets
3. kubectl create secret generic db-cred-special --from-literal=username=mohammedshaik81 --from-literal=password=$\!B\\*d\$zDsB
4. kubectl get secrets
5. echo -n 'mohammedshaik81' | base64 -> bW9oYW1tZWRzaGFpazgx
6. echo -n '12345' | base64 -> MTIzNDU=
7. kubectl create -f secrets-manifest.yml
8. kubectl get secrets
9. kubectl describe secret secret-name
10. kubectl get secret secret-name -o yaml/json
11. echo 'base64string' | base64 --decode

Lab: Secrets In Kubernetes
1. chmod 777 podvolumesecrets.yml
2. kubectl create -f podvoumesecrets.yml
3. kubectl get pods
4. kubectl describe pod podname
5. kubectl exec -it podname -- /bin/bash
6. cd /etc/foo/
7. cat username
8. cat password

Service Discovery Using DNS
1. See pdf file.

Lab: Service Discovery Using DNS
//Executed the backend
1. chmod 777 mysql-database.yml
2. kubectl create -f mysql-database.yml
3. kubectl logs database(podname) //These are not live logs
4. kubectl logs --follow database //To see live logs 
5. kubectl get pods
6. kubectl exec -it database -- mysql -u root -p //To go inside the mysqldb 
7. Enter the password defined in the yml file.
8. use database-name
9. show databases;
10. show tables;

//Execute the backend service to connect front end
1. chmod mysqldb-service.yml
2. kubectl create -f mysqldb-service.yml
3. kubectl get services

//Execute the application
1. chmod 777 nodejs-web.yml
2. kubectl create -f nodejs-web.yml
3. kubectl describe deployment deployment-name
4. kubectl get pods
5. kubectl logs --follow podname


//Execute the service for the front end to get the application url
1. chmod 777 nodejs-service.yml
2. kubectl create -f nodejs-service.yml
3. kubectl get services
4. minikube service nodejs-web-service --url

//To see the data stored through the application
1. kubectl exec database -it -- mysql -u root -p
2. use helloworld
3. show tables;
4. select * from vistits;

Config Map In Kubernetes
- See pdf file.

Lab: ConfigMap In Kubernetes
//Create configmap from the file
1. kubectl create configmap front-end-config --from-file=front-end-config
2. kubectl describe configmap front-end-config

//Create configmap from the literal
3. kubectl create configmap some-config --from-literal=front.size=14px --from-literal=color.default=green
4. kubectl get configmap some-config -o yaml

//Create configmap from the yml file
5. kubectl create -f simple-configmap.yml
6. kubectl get configmap simple-configmap -o yaml

//Use configmap in containers
7. kubectl create -f demo-web-pod.yml
8. kubectl describe pod demo-web
9. kubectl exec -it demo-web -- /bin/bash
10. printenv STRATEGY_RISK //To print any environment variable

//In case where you have multiple key value pairs or multiple configuration assign to your pod in that case u can't defnine
each and every configuration in you yml file and execute the container bcoz this will create a lot of dependency and errorprone.
Hence we use POSIX environment variables
11. kubectl create -f posix-config.yml
12. kubectl get configmap name -o json
13. kubectl create -f posix-pod-demo.yml 
14. kubectl describe pod podname
15. kubectl exec -it demo-posix-env -- /bin/bash
16. env // You will get the complete environment variables of the container

//Use configMap in container volumes
1. kubectl create -f demo-config-pod.yml
2. kubectl describe pod podname
3. kubectl exec -it podname -- /bin/bash
4. cd /etc/config/
5. ls
6. cat strategy.type

Ingress In Kubernetes
- See pdf file.

Lab: Ingress In Kubernetes
1. kubectl create -f nginx-deployment.yml
2. kubectl get deployment
3. kubectl describe deployment deployment_name
4. kubectl create -f nginx-deployment-service.yml
5. kubectl create -f magicalnginx-deployment.yml
6. kubectl create -f magicalnginx-service.yml
7. kubectl get services
8. minikube service service-name --url
9. curl --url
10. kubectl create -f ingreess-controller.yml
11. kubectl get ingress
12. kubectl describe  ingress nginx-rules
13. minikube ip
14. curl ip -H 'Host: nginx-official.example.com'

Volume In Kubernetes
- Kubernetes support 12 to 15 kind of volumes. You can google for more details on volumes.
- See the pdf file for more details.

EmptyDir Volume In Kubernetes
1. kuebctl create -f emtprydir-pod.yml
2. kuebctl get pods
3. kubectl exec -it redis --/bin/bash
4. cd /data/redis/
5. echo "Hello this is mohammmed sibgathulla" >> test_file.txt
6. ls
7. cat test_file.txt
8. apt-get update
9. apt-get install procps
9. ps aux // To identify the running processes
10. kubectl get pod redis --watch //To watch the status of pod (Open it in another terminal)
11. kill -9 1 or kill 1(In previous terminal) Here 1 is the process id & here we are restarting the container
12. kubectl exec -it redis -- /bin/bash
13. cd /data/redis/ You can see the file test_file.txt it will not be deleted. If we remove the container that data will be erased.


HostPath Volume In Kubernetes
1. minikube status
2. minikube ssh
3. pwd
4. id -un
5. mkdir /home/docker/data
6. cd /home/docker/data/
7. pwd
8. vi index.html 
9. ls
10. ls -lrt
11. sudo chmod 777 index.html
12. ls -lrt
13. vi index.html //Enter some content
14. cat index.html
15. exit
16. kubectl create -f hostpath-pv.yml //Here there is 10 GB Persistent Volume
17. kubectl get persistentvolume pv-local
18. kubectl create -f hostpath-pvc.yml //Herer we are claimin 5 GB of 10 GB Persistent Volume
19. kubectl get persistentvolumeclaim hostpath-pvc 
20. kubectl create -f apache-deployment.yml
21. kubectl get pods
22. kubectl expose deployment httpd --type=NodePort
23. minikube service httpd --url
24. curl url
25. kubectl get pods
26. kubectl exec -it podname -- /bin/bash
27. curl localhost:80 
28. apt-get update
29. apt-get install curl
30. curl localhost:80 

Dynamic Volume Provisioning In Kubernetes
1. See the pdf file.

Dynamic Volume: Run Wordpress On Kubernetes
1. minikube status
2. minikube stop
3. Setup AWS IAM permission for Kops
Permissions required for Kops user.
AmazonEC2FullAccess
AmazonRoute53FullAccess
AmazonS3FullAccess
IAMFullAccess
AmazonVPCFullAccess
//Aws Console
4. Go to Service -> IAM
5. Go to Users -> Add a User -> kops (username)
6. Select programatic access.
7. Create a new group -> kopsUserGroup (group name) -> Select Adminstrator Access
8. Add Tags -> key -> user & value -> kops (You can put anthing here) -> create user.
9. You will get Access Key Id & Secret Key

//In your local terminal
10. aws configure
11. Enter the Access Key Id & Secret Key
12. Default region name: Press Enter
13. Default output format: Press Enter
14. ls -lrt ~/.aws/      //To verify configuration
15. cat ~/.aws/credentials //To verify configuration

//Create S3 bucket for KOPST_STATE_STORE
//Aws Console
16. Service -> Storage -> S3 -> Create Bucket -> kops-storage-b3459235 (Name //U can give any name)
17. www.cloudping.info -> choose the region
18. Select the region
19. Add tags -> user & kops //random
20. Uncheck block public access.
21. Create bucket.

//Setup DNS for the Kops Cluster
22. Services -> Networking & Content Delivery -> Route 53
23. Dnsmanagement -> Create Hosted Zone -> Enter domain name -> Create.
24. You will get the NS records for your domain.
25. Update those NS records in the domain registrar console panel.

Kubernetes Setup On AWS Cloud
1. ssh-keygen -f .ssh/id_rsa //hit enter twice after this
2. cat .ssh/id_rsa.pub
3. Rename kops-linux-amd64 to kops if the directory is present 
4. cd /usr/local/bin/
5. sudo mv /usr/local/bin/kops-linux-amd64 /usr/local/bin/kops If it is not present skip 3, 4, & 5.

//Create a Kubernetes Cluster
6. kops create cluster --yes --state=s3://kops-storage-b3459235 --zones=ap-south-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --name=fabulouzfashion.com --ssh-public-key ~/.ssh/id_rsa.pub //Here you haved added --node-count=2 it is optional if you don't specify 3 are created by default.

If you have a domain
6. kops create cluster --yes --state=s3://kops-storage-b3459235 --zones=ap-south-1a --node-size=t2.micro --master-size=t2.micro --name=fabulouzfashion.com
If you don't have domain
6. kops create cluster --yes --state=s3://kops-storage-b3459235 --zones=ap-south-1a --node-size=t2.micro --master-size=t2.micro --name=test.k8s.local

//You can use any of the above according to your requirement.

8. kops validate cluster --state=s3://kops-storage-b3459235 
   kops validate cluster --state=s3://kops-storage-b3459235 -o json /yaml
//The above command may show Validation Failed. The validation of the kubernetes cluster will take 10 to 15 minutes. You have to wait and then run the command again.

9. In EC2 Dashboard you can see 5 volumes

//In your local terminal
10. kubectl create -f storageclass.yml -> storageclass.storage.k8s.io/aws-standard created
11. kubectl describe storageclass.storage.k8s.io/aws-standard
//In the static you need to create the storage volume and then u need to claim that particular storage volume.
But in the dynamic one you can create the object and that volume will be created at runtime when you will claim it. It will save your cost as well. Bcoz on cloud nothing is free. If you are creating 100 GB database you have occupied 100 GB Disk before creating the persistence volume claim. Suppose you want to deploy your system and it will be deployed tomorrow and today your are just doing the preparation as pre maintenance and u are creating the volume today so you are providing 24 hrs extra billing to the cloud service provider for that particular disk. So dynamic volume creation is always preferred. 

12. kubectl create -f pv-claim.yml
13. kubectl describe persistentvolumeclaim/mysqldb-storage

//Now if you go and see in the aws console you will have 6 volumes and a new volume will be create with name as fabulouzfashion.com-dynamic-pvc-5dc00110-97ff-4c35-b08c-2ad2cc7b13d3

14. kubectl create -f secrets.yml
15. kubectl create -f mysqldb.yml -> replicationcontroller/wordpress-db 
16. kubectl describe replicationcontroller/wordpress-db 
18. kubectl get pods
19. kubectl logs --follow wordpress-db-6f2tl
20. kubectl get pod
21. kubectl create -f mysqldbservice.yml
22. kubectl create -f wordpress.yml
23. kubectl describe deployment.apps/wordpress-deployment
24. kubectl get pod
25. kubectl create -f wordpress-service.yml

//Now if you go and see in the aws console you can see that a new Load Balancer will be created.
If you don't have any domain name then you can copy the DNS Name in the Load Balancer description and access wordpress i.e a7dbafcb62dab4978840ae8df049a48f-1616729049.ap-south-1.elb.amazonaws.com
If you are using the domain name fabulouzfashion.com
26. Route53 service -> Hosted Zones -> Selcect Your Created Hosted Zone -> Create Record Set -> Name = fabwordpress -> Alias = Yes -> Alias Target = Load Balncer Id (...) -> Create
28. Install Wordpress
29. No you can access the website at fabwordpress.fabulouzfashion.com

//In your local terminal
30. kubectl get pods
31. kubectl delete pod wordpress-db-6f2tl // we are deleting the pod of mysql db, but a new container will be created.
//Again you refresh the website the content will be the same bcoz the data which has created in mysql database is present on the mounted location on the Aws. It is not container dependent right now
//So we have seperated the persistence disk from stateful application to the mounted point which is on the Aws.
//Untill and unless we will delete our persistent volume claim this data will not be erased. Once we delete the persistent volume claim the data will be deleted. Hence when we are working in organizatin we need to ensure what is the Retintion / Reclaim Policy of your Storage Class. If your Storage Class Reclaim Policy is default it is not retained. Then we have to make sure that we are not deleting any Persistent Volume Claim otherwise you will loose your whole data. 

32. kops delete cluster --name=fabulouzfashion.com --yes --state=s3://kops-storage-b3459235 //It will delete all the deployments from your aws cluster.
33. kops delete cluster --name=test.k8s.local --yes --state=s3://kops-storage-b3459235 

Creating a Service-Linked Role (Console)

Before you create a service-linked role in IAM, find out whether the linked service automatically creates service-linked roles, In addition, learn whether you can create the role from the service's console, API, or CLI.

To create a service-linked role (console)

    Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/

.

In the navigation pane of the IAM console, choose Roles. Then choose Create role.

Choose the AWS Service role type, and then choose the service that you want to allow to assume this role.

Choose the use case for your service. If the specified service has only one use case, it is selected for you. Use cases are defined by the service to include the trust policy required by the service. Then choose Next: Permissions.

Choose one or more permissions policies to attach to the role. Depending on the use case that you selected, the service might do any of the following:

    Define the permissions used by the role

    Allow you to choose from a limited set of permissions

    Allow you to choose from any permissions

    Allow you to select no policies at this time, create the policies later, and then attach them to the role.

Select the box next to the policy that assigns the permissions that you want the role to have, and then choose Next: Tags.
Note

The permissions that you specify are available to any entity that uses the role. By default, a role has no permissions.

Choose Next: Review. You cannot attach tags to service-linked roles during creation. For more information about using tags in IAM, see Tagging IAM Users and Roles.

For Role name, the degree of role name customization is defined by the service. If the service defines the role's name, then this option is not editable. In other cases, the service might define a prefix for the role and allow you to type an optional suffix.

If possible, type a role name suffix to add to the default name. This suffix helps you identify the purpose of this role. Role names must be unique within your AWS account. They are not distinguished by case. For example, you cannot create roles named both <service-linked-role-name>_SAMPLE and <service-linked-role-name>_sample. Because various entities might reference the role, you cannot edit the name of the role after it has been created.

(Optional) For Role description, edit the description for the new service-linked role.

Review the role and then choose Create role.

Pod Presets In Kubernetes
- You can use multiple persets on a single pod & viceversa.
- See the pdf file.

Enable & Execute PodPresets In Kubernetes
1. kops create cluster --yes --state=s3://kops-storage-b3459235 --zones=ap-south-1a --node-size=t2.micro --master-size=t2.micro --name=fabulouzfashion.com
   kops create cluster --yes --state=s3://kops-storage-b3459235 --zones=ap-south-1a --node-size=t2.micro --master-size=t2.micro --name=test.k8s.local
2. kops validate cluster --state=s3://kops-storage-b3459235
3. kops get cluster --state=s3://kops-storage-b3459235                   #It will display the cluster name
4. kops edit cluster --state=s3://kops-storage-b3459235 test.k8s.local
5. Search for kubeAPIServer -> /kubeAPIServer Press enter -> Pattern not found: kubeAPIServer
6. You copy paste the content from spec file in the same line of topology.
7. kops update cluster --state=s3://kops-storage-b3459235 test.k8s.local
8. update edit cluster --state=s3://kops-storage-b3459235 test.k8s.local --yes
9. kops rolling-update cluster --state=s3://kops-storage-b3459235 //It will restart all the kubernetes nodes one by one.
10. kops rolling-update cluster --state=s3://kops-storage-b3459235 --yes
11. kubectl create -f mysql-pod-preset.yml
12. kubectl describe name
13. kubeclt create -f mysql-pod.yml
14. kubectl describe name
15. cp mysql-pod.yml mysql-db.yml
16. ls
17. vi mysql-db.yml
apiVersion: v1
kind: Pod
metadata:
    name: mysql-test
    labels:
        app: mysql-server
        role: mysql-database
spec:
    containers:
        - name: mysql
          image: mysql:8.0
          command: ["mysqld"]

18. kubectl create -f mysql-db.yml
19. kubectl describe name //You will get the same environmental variable as in mysql-pod. It means the admission controller is working fine. All the changes which are present in the existing pod preset will also reflect in the new pod it will create with the same label and the same selector. We have seen the use of the Podpreset and the Admission Controller.    

Stateful Sets In Kubernetes
- See the pdf file.

Deploy Cassandra In StateFul Set
1. minikube stop
//Cassandra Database requirest atleast 1 GB RAM to spin up. You have to choose t2.small
2. kops create cluster --yes --state=s3://kops-storage-b3459235 --zones=ap-south-1a --node-size=t2.small --master-size=t2.small --name=fabulouzfashion.com
3. kops validate cluster --state=s3://kops-storage-b3459235
4. kubectl create -f cassandra-statefulset.yml
5. kubectl get pods
5. kubectl get pods -w -l statefulset.apps/cassandra  //Monitor the pods livestate during the deployment
7. watch kubectl get pod //To see live status of your pod
8. kubectl edit pod //In insert mode change the replicas to 2.
9. kubectl get pv //To see the persistent volumes
10. kubectl exec -it cassandra-0 -- nodetool status
11. kubectl get pods
12. kubectl delete pod cassandra-1
13. kubectl exec -it cassandra-0 -- nodetool status
14. kubectl get pods //The deleted pod will spin up again with same name. If you do the same with the replication controller or replicaset or deployment then you will get some new name of the particular pod which was deleted.
15. kubectl edit StatefulSet cassandra // In Insert mode at replicas edit to 0 In order to delete the StatefulSet 
16. kubectl get pods //If you delete the Stateful Set directly then it will not gurantee that the pods will be deleted.
17. kubectl get StatefulSet
18. kubectl delete StatefulSet cassandra //But in the aws all the volumes will be there in avaliable state. Hence if you want to remove the volumes 
19. kubectl get pvc
20. kubectl delete pvc volname
21. kubectl delete pvc --all //To delete all the volumes
22. kops delete cluster --state=s3://kops-storage-b3459235 fabulouzfashion.com --yes

DaemonSet In Kubernetes
- See the pdf file.

Lab: DaemonSet In Kubernetes
1. kops create cluster --yes --state=s3://kops-storage-b3459235 --zones=ap-south-1a --node-size=t2.micro --master-size=t2.micro --name=fabulouzfashion.com
2. kops validate cluster --state=s3://kops-storage-b3459235
3. kubectl get nodes
4. kubectl create -f daemonset.yml
5. kubectl get pods //3 pods will be created each for single node.
6. kops delete cluster --state=s3://kops-storage-b3459235 fabulouzfashion.com --yes
Points to remember
# When a DaemonSet is created the selector cannot be changed.
# You must speciyf a pod selector that matches the labels of template
# You shoud not create any pods whose labels match this selector, either directly, via another DaemonSet, or via other controller such as ReplicaSet. Otherwise, DaemonSet controller will think that those Pods were created by it. 

AutoScaling In Kubernetes
- See the pdf file.

Lab: AutoScaling In Kubernetes
1. kops create cluster --yes --state=s3://kops-storage-b3459235 --zones=ap-south-1a --node-size=t2.micro --master-size=t2.micro --name=fabulouzfashion.com
2. kops validate cluster --state=s3://kops-storage-b3459235
3. kubectl get service,hpa,pod,deployment -owide //it will show you the status of services running on your cluster the hpa
4. watch -n1 !!
5. kubectl create -f autoscaling.yml
6. kubectl autoscale deployment hpa-example --min=2 --max=10 --cpu-percent=50
7. kubectl describe hpa //you will get an error
8. kubectl get hpa
8. kubectl delete -n kube-system deployments.apps metrics-server //delete existing metrics-server if any
9. git clone https://github.com/kubernetes-incubator/metrics-server.git
10. kubectl delete hpa
11. kubectl delete deployment

#The above example is not working.

Affinity In Kubernetes
- See the pdf file.

Lab: Affinity In Kubernetes
1. kops create cluster --yes --state=s3://kops-storage-b3459235 --zones=ap-south-1a --node-size=t2.micro --master-size=t2.micro --name=fabulouzfashion.com
2. kops validate cluster --state=s3://kops-storage-b3459235
3. kubectl get nodes --show-labels
4. kubectl label nodes ip-172-20-33-27.ap-south-1.compute.internal disk-type=ssd //To tag or label your particular node
5. kubectl describe node ip-172-20-33-27.ap-south-1.compute.internal
6. kubectl create -f nodeselector.yml
7. kubectl describe pod-name
8. kubectl create -f node-affinity.yml
9. kubectl describe pod-name
10. opeartor: Not-In //You have to update in node-affinity.yml for anti affinity

Architecture & Working Model Of Master Component
- See the pdf file.

Manage Resource Quota In Kubernetes
- See the pdf file.

Resource Quota In Kubernetes
- See the pdf file.

Lab: Resource Quota In Kubernetes
1. kops validate cluster --state=s3://kops-storage-b3459235
2. kubectl get namespaces
3. kubectl get namespaces --show-labels
4. kubectl create -f resource-quota.yml
5. kubectl get namespaces
6. kubectl get resourcequota
7. kubectl get resourcequota --namespace=mynamespace
8. kubectl create -f dep-without-quota.yml
9. kubectl get deployment --namespace=mynamespace
10. kubectl describe deployment --namespace=mynamespace //None of the container will be deployed
11. kubectl get rs --namespace=mynamespace
12. kubectl describe rs --namespace=mynamespace //failed quota: user-compute-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
13. kubectl create -f dep-with-quota.yml
14. kubectl get deployment --namespace=mynamespace //1 replica failed
15. kubectl get rs --namespace=mynamespace
16. kubectl describe rs --namespace=mynamespace //forbidden: failed quota: user-compute-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory (Here quota is exceeding we have specified 1 Gi memory but here for 3 replicas it is 1.5 Gi so one replica failed)
17. kubectl get quota user-compute-quota --namespace=mynamespace
18. kubectl describe quota user-compute-quota --namespace=mynamespace
19. kubectl delete deployment helloworld-deployment --namespace=mynamespace
20. kubectl describe quota user-compute-quota --namespace=mynamespace
21. kubectl create -f default-limit.yml 
22. kubectl describe limitrange/limits-quota --namespace=mynamespace
23. kubectl delete deployment nginx-deployment --namespace=mynamespace
24. kubectl create -f dep-without-quota.yml 
25. kubectl get deployment --namespace=mynamespace

User Management In Kubernetes
- See the pdf file.

Role Base Access Control (RBAC) In Kubernetes
- See the pdf file.

Role Base Access Control Implementation
- RBAC works only on Kubernetes Cluster 1.9+
- RBAC is not enabled by default on Cluster. User needs to enable it.
1. kubectl api-versions
2. kops create cluster --yes --state=s3://kops-storage-b3459235 --zones=ap-south-1a --node-size=t2.micro --master-size=t2.micro --authorization=RBAC --name=fabulouzfashion.com
3. minikube start --extra-config=apiserver.Authorization.MODE=RBAC
4. kops validate cluster --state=s3://kops-storage-b3459235
5. kubectl get namespace
6. kubectl create -f rbac-role.yml
7. kubectl config get-contexts //Till now we are using the admin config file
//Retrieve Keys From KOPS
8. aws s3 sync s3://kops-storage-b3459235/fabulouzfashion.com/pki/private/ca/ ca-key
9. aws s3 sync s3://kops-storage-b3459235/fabulouzfashion.com/pki/issued/ca/ ca-crt
10. ls 
11. cd ca-crt -> ls
12. mv ca-key/*.key ca.key -> ls
13. mv ca-crt/*.crt ca.crt -> ls //Now we can apply these keys and certificates with any user we want.
//Create new user
14. sudo apt install openssl
15. openssl genrsa -out peter.pem 2048 -> ls
16. openssl req -new -key peter.pem -out peter-csr.pem -subj "/CN=peter/O=myteam/"
17. openssl x509 -req -in peter-csr.pem -CA ca.crt -CAkey ca.key -CAcreateserial -out peter.crt -days 10000
//Add new context
18. kubectl config set-credentials peter --client-certificate=peter.crt --client-key=peter.pem
19. kubectl config set-context peter --cluster=fabulouzfashion.com --user peter
20. kubectl config get-contexts
21. kubectl get nodes
22. kubectl config use-context peter
23. kubectl get nodes
24. kubectl get pods
25. kubectl get deployments
26. kubectl get pods --namespace=mynamespace
27. kubectl get deployments --namesapce=mynamespace
28. kubectl get rs --namespace=mynamespace
29. kubectl create -f deployment.yml
30. kubectl get pods --namespace=mynamespace
31. kubectl config get-contexts
32. kubectl config use-context fabulouzfashion.com
33. kubectl create -f deployment.yml //here the namespace should be edited to default
34. kubectl get pods
35. kubectl get pods -A //To see all the pods in all the namespaces
36. kubectl config use-context peter
37. kubectl get pods --namespace=mynamespace
38. kubectl get pods -A //It will not work here. Error from server (Forbidden): pods is forbidden: User "peter" cannot list resource "pods" in API group "" at the cluster scope
39. kubectl delete pod nginx-deployment-5f847d84b8-8zbjg //forbidden: User "peter" cannot delete resource "pods" in API group "" in the namespace "mynamespace"

Networking In Kubernetes
- See the pdf file.

Node Management
1. kops validate cluster --state=s3://kops-storage-b3459235
2. kubectl get nodes
3. kubectl get pods
4. kubectl get pods -o wide
5. kubectl get nodes
6. kubectl drain ip-172-20-51-30.ap-south-1.compute.internal --ignore-daemonsets=false
7. kubectl get pods
8. kubectl get pods -o wide //All the pods are scheduled on other nodes but no pod is scheduled on the drained node. Now u can do maintenance on the drained node.
9. kubectl uncordon ip-172-20-51-30.ap-south-1.compute.internal //After maintenance u can put the node back

Kubernetes In Production: High Availability
- See the pdf file.

Demo: Kubernetes HA Deployment
1. kops create cluster --yes --state=s3://kops-storage-b3459235 --zones="ap-south-1a,ap-south-1b" --node-size=t2.micro --master-size=t2.micro --master-count 3 --node-count 5 --authorization=RBAC --name=fabulouzfashion.com --dns-zone=fabulouzfashion.com
2. kops validate cluster --state=s3://kops-storage-b3459235
3. kubectl get nodes -A
4. kubectl get nodes -o wide
5. kubectl create -f nginx-deployment.yml
6. kubectl get svc / service
7. kubectl describe svc service-name //copy the load balancer url and hit it in the browser u can configure the url in the hosted zone
8. kubectl get pods -o wide
9. kubectl create -f live-chat.yml //One more load balancer will spin up
10. kubectl get pods 
11. kubectl get svc
12. kuebctl describe svc chat //copy the load balancer url and hit it in the browser u can configure the url in the hosted zone
13. kops delete cluster --state=s3://kops-storage-b3459235 fabulouzfashion.com --yes 

HELM: Introduction
- See the pdf file.

Installing & Running HELM On Kubernetes
1. kops create cluster --yes --state=s3://kops-storage-b3459235 --zones=ap-south-1a --node-size=t2.micro --master-size=t2.micro --authorization=RBAC --name=fabulouzfashion.com
2. kops validate cluster --state=s3://kops-storage-b3459235
3. kubectl api-versions
4. wget https://storage.googleapis.com/kubernetes-helm/helm-v2.14.1-linux-amd64.tar.gz
5. chmod 655 helm-v2.14.1-linux-amd64.tar.gz
5. tar -xzvf helm-v2.14.1-linux-amd64.tar.gz
6. cd linux-amd64/ -> ls cat README.md -> cd ..
7. sudo mv linux-amd64/helm /usr/local/bin/helm
8. cd /usr/local/bin/ -> ls
9. helm -h 
10. kubectl create -f helm-rbac.yml
11. helm init --service-account helm-tiller 
12. kubectl get pods -n kube-system 
//The above way didn't worked


//Alternate Way of Installing Helm
1. kubectl cluster-info
2. kubectl config get-contexts
3. kubectl config use-context context-name
4. curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > install-helm.sh
5. chmod u+x install-helm.sh
6. ./install-helm.sh  //You will get the output as helm installed into /usr/local/bin/helm Run 'helm init' to configure helm.
7. kubectl -n kube-system create serviceaccount tiller
8. kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
9. helm init --service-account tiller
10. kubectl get pods --namespace kube-system

1. helm search <Application Name>nginx/redis
2. helm install <Application Name>stable/redis
3. helm install --name <releasename> <ApplicationName> 
4. helm ls
5. helm install --name prodredishelm stable/redis
6. kubectl get secret --namespace default prodredishelm -o jsonpath="{.data.redis-password}" | base64 --decode //It will display the password (rilYrv7BzF)
7. kubectl run --namespace default prodredishelm-client --rm --tty -i --restart='Never' --env REDIS_PASSWORD=rilYrv7BzF --image docker.io/bitnami/redis:5.0.7-debian-10-r32 -- bash
8. echo $REDIS_PASSWORD
9. redis-cli -h prodredishelm-master -a $REDIS_PASSWORD
10. help
11. set mohammed softwaredeveloper
12. exit
13. kubectl get pods
14. kubectl get pvc
15. helm ls
16. helm delete prodredishelm
17. kubectl get all

Create And Deploy Helm Chart On Cluster
1. kops create cluster --yes --state=s3://kops-storage-b3452357 --zones=ap-south-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --name=fabulouzfashion.com --ssh-public-key ~/.ssh/id_rsa.pub 
2. kops validate cluster --state=s3://kops-storage-b3452357
1. helm create <chart-name>
2. helm create hello-world
3. tree hello-world/
4. helm lint <chart-full-path>
5. helm lint hello-world/
6. helm template <chart-full-path>
7. helm template hello-world/
8. helm install --name <release-name> <chart-full-path>
9. helm install --name nginxtest hello-world/
10. export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=hello-world,app.kubernetes.io/instance=testnginx" -o jsonpath="{.items[0].metadata.name}")
11. echo $POD_NAME
12. kubectl port-forward $POD_NAME 8080:80
13. bg
14. curl http://127.0.0.1:8080
15. fg
//Change version number in chart.yml
//Change the replicast to 2 in values.yml & instead of ClusterIp put LoadBalancer
16. helm ls --all / helm ls
17. helm upgrade <release-name> <chart-full-path>
18. helm upgrade nginxtest hello-world/
19. kubectl get --namespace default svc -w nginx-hello-world -> Copy paste the load balancer ip in the browser
20. helm ls
21. helm delete --purge <release-name>
22. helm delete nginxtest
23. kubectl get all
24. helm rollback <release-name> <release-version>
25. kops delete cluster --state=s3://kops-storage-b3452357 fabulouzfashion.com --yes 

Upload HELM Chart In S3 Bucket
1. kops validate cluster --state=s3://kops-storage-b3452357
2. aws sts get-caller-identity
3. vi s3-create-bucket.sh
4. chmod 777 s3-create-bucket.sh
5. ./s3-helm-repo.sh
6. helm repo list
7. export AWS_REGION=ap-south-1
8. echo $AWS_REGION
9. ls
10. helm package hello-world
11. helm s3 push <tar-file> <s3chart-name>
12. helm s3 push hello-world-2.1.0.tgz anshul-charts
13. helm search <repo-name>
14. helm search hello-world
15. helm install anshul-charts/hello-world
16. kops delete cluster --state=s3://kops-storage-b3452357 fabulouzfashion.com --yes 

Serverless On Kubernetes
- See the pdf file.

Kubeless Introduction
- See the pdf file.





 





































 








